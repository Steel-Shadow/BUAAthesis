% !Mode:: "TeX:UTF-8"

\chapter{问题描述}

大模型的推理速度慢，限制了它的实时应用。这样的低效推理在自回归生成任务（机器翻译、文本总结、语言模型等）中尤为显著。自回归生成任务需要多次迭代运行，每个标记（token）的生成都依赖于之前生成的标记序列。推理过程串行化，推理过程的内存带宽受限，硬件利用率差，推理速度慢。

在自回归生成任务中的第 $n$ 轮解码迭代中，小模型和大模型分别将已经生成的文本 $y_{1:n-1}=(y1,\dots,y_{n-1})$ 作为输入，生成下一个标记的概率分布 $p_S(y|y_{1:n-1})$ 和 $p_L(y|y_{1:n-1})$。

\begin{align}
    y_{n,S} & \sim p_S(y|y_{1:n-1}) \\
    y_{n,L} & \sim p_L(y|y_{1:n-1})
\end{align}

仅当小模型可能作出不准确预测时，才需要大模型生成文本，该判别函数为$\pi(y_{1:n-1})$：

\begin{equation}
    y_n= \begin{cases}y_{n, S} & \text { if } \pi\left(y_{1: n-1}\right)=0 \\ y_{n, L} & \text { if } \pi\left(y_{1: n-1}\right)=1\end{cases}
\end{equation}

为了提高推理速度，需要设计一个轻量的判别函数 $\pi$，只在必要时使用大模型。

当大模型接管控制权并预测标记 $y_n$ 时，它可以并行接收多个输入标记 $y_{1:n-1}$，实现非自回归并行推理。这样虽然没有减少浮点运算量(FLOP)，但是提高了解码过程的算术强度和硬件利用率，实现了推理加速。
